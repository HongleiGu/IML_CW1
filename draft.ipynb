{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d87647",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Step 1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebc426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60733a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8) (2000, 8)\n"
     ]
    }
   ],
   "source": [
    "# load the clean and noisy data\n",
    "# sample data: -59\t-55\t-62\t-64\t-68\t-77\t-86\t1\n",
    "# the last number is the label\n",
    "\n",
    "clean_data = np.loadtxt(\"./wifi_db/clean_dataset.txt\")\n",
    "noisy_data = np.loadtxt(\"./wifi_db/noisy_dataset.txt\")\n",
    "\n",
    "print(clean_data.shape, noisy_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb46b4",
   "metadata": {},
   "source": [
    "# Tree building\n",
    "\n",
    "So we now build the tree, following the spec, each node has attributes: \n",
    "\n",
    "```\n",
    "{\"attribute\", \"value\", \"left\", \"right\", \"leaf\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce7211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "  def __init__(self, attribute, value, left=None, right=None, leaf=False):\n",
    "    self.attribute = attribute\n",
    "    self.value = value\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    self.leaf = leaf\n",
    "\n",
    "\n",
    "def get_entropy(dataset):\n",
    "    # get the count for each label\n",
    "    labels, counts = np.unique(dataset[:, -1], return_counts=True)\n",
    "    # turn count to probs\n",
    "    probs = counts / np.sum(counts)\n",
    "    # recall H(d) = \\sum_{k=1}^{k=K}p_k*\\log_2(p_k)\n",
    "    return -np.sum(probs * np.log2(probs + 1e-9))  # the 1e-9 avoids log(0)\n",
    "\n",
    "\n",
    "def find_split(dataset):\n",
    "    # we -1 since last col is label\n",
    "    num_data, num_features = dataset.shape[0], dataset.shape[1] - 1\n",
    "    # dummy init\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_left_split = None\n",
    "    best_right_split = None\n",
    "    min_entropy = float('inf')\n",
    "    # iterate all the features\n",
    "    for i in range(num_features):\n",
    "        # as per spec, sort it, note this sort in increasing order\n",
    "        sorted_dataset = dataset[dataset[:, i].argsort()]\n",
    "        # traverse to find the best splitting point\n",
    "        for j in range(1, num_data - 1):\n",
    "            left_split, right_split = sorted_dataset[:j, :], sorted_dataset[j:, :]\n",
    "            l_entropy = get_entropy(left_split)\n",
    "            r_entropy = get_entropy(right_split)\n",
    "            total_entropy = (j * l_entropy + (num_data - j) * r_entropy) / num_data\n",
    "\n",
    "            if total_entropy < min_entropy:\n",
    "                min_entropy = total_entropy\n",
    "                best_feature = i\n",
    "                best_threshold = sorted_dataset[j, i]\n",
    "                best_left_split = left_split\n",
    "                best_right_split = right_split\n",
    "\n",
    "    return best_feature, best_threshold, min_entropy, best_left_split, best_right_split\n",
    "\n",
    "\n",
    "def decision_tree(training_dataset, depth):\n",
    "  # get unique labels\n",
    "  labels = np.unique(training_dataset[:, -1])\n",
    "  # base case: if dataset is empty or all labels are identical\n",
    "  if training_dataset.size == 0 or labels.shape[0] == 1:\n",
    "    return (Node(attribute=None, value=labels[0] if labels.size > 0 else None, leaf=True), depth)\n",
    "  else:\n",
    "    # find the best feature and split\n",
    "    best_feature, best_threshold, min_entropy, best_left_split, best_right_split = find_split(training_dataset)\n",
    "\n",
    "    # edge case: if we cannot find a valid split (e.g., empty branch)\n",
    "    if best_left_split is None or best_right_split is None or \\\n",
    "       best_left_split.size == 0 or best_right_split.size == 0:\n",
    "        # fallback: create a leaf node with the majority label\n",
    "        values, counts = np.unique(training_dataset[:, -1], return_counts=True)\n",
    "        majority_label = values[np.argmax(counts)]\n",
    "        return (Node(attribute=None, value=majority_label, leaf=True), depth)\n",
    "\n",
    "    # Recursive calls\n",
    "    l_branch, l_depth = decision_tree(best_left_split, depth + 1)\n",
    "    r_branch, r_depth = decision_tree(best_right_split, depth + 1)\n",
    "\n",
    "    # create internal node\n",
    "    node = Node(attribute=best_feature, value=best_threshold, left=l_branch, right=r_branch)\n",
    "    return (node, max(l_depth, r_depth))\n",
    "tree, depth = decision_tree(clean_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68b5bf",
   "metadata": {},
   "source": [
    "# predicting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b69f6318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(node, sample):\n",
    "    # dfs in a binary search tree-like structure\n",
    "    # if leaf, then just return the value\n",
    "    if node.leaf:\n",
    "        return node.value\n",
    "    # if feature value <= threshold, go left; else, go right\n",
    "    if sample[node.attribute] <= node.value:\n",
    "        return predict(node.left, sample)\n",
    "    else:\n",
    "        return predict(node.right, sample)\n",
    "\n",
    "\n",
    "def predict_batch(root, dataset):\n",
    "    # predict labels for all rows in a dataset\n",
    "    preds = []\n",
    "    for sample in dataset:\n",
    "        preds.append(predict(root, sample))\n",
    "    return np.array(preds)\n",
    "predict(tree, noisy_data[0, :-1])\n",
    "# and yes it is, in room 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbd8bf",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56a7dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold $1: accuracy = 1.000000, tree depth = 13\n",
      "fold $2: accuracy = 1.000000, tree depth = 13\n",
      "fold $3: accuracy = 0.970000, tree depth = 13\n",
      "fold $4: accuracy = 0.945000, tree depth = 13\n",
      "fold $5: accuracy = 0.980000, tree depth = 13\n",
      "fold $6: accuracy = 0.960000, tree depth = 13\n",
      "fold $7: accuracy = 0.955000, tree depth = 13\n",
      "fold $8: accuracy = 0.965000, tree depth = 13\n",
      "fold $9: accuracy = 0.985000, tree depth = 13\n",
      "fold $10: accuracy = 0.995000, tree depth = 13\n",
      "best accuracy: 0.995\n",
      "\n",
      "\n",
      "===== I am a separation line =====\n",
      "\n",
      "\n",
      "fold $1: accuracy = 0.905000, tree depth = 13\n",
      "fold $2: accuracy = 0.915000, tree depth = 13\n",
      "fold $3: accuracy = 0.905000, tree depth = 13\n",
      "fold $4: accuracy = 0.930000, tree depth = 13\n",
      "fold $5: accuracy = 0.860000, tree depth = 13\n",
      "fold $6: accuracy = 0.865000, tree depth = 13\n",
      "fold $7: accuracy = 0.900000, tree depth = 13\n",
      "fold $8: accuracy = 0.875000, tree depth = 13\n",
      "fold $9: accuracy = 0.910000, tree depth = 13\n",
      "fold $10: accuracy = 0.880000, tree depth = 13\n",
      "best accuracy: 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.Node at 0x1c68fcd9b10>,\n",
       " [0.25, 0.905, 0.915, 0.905, 0.93, 0.86, 0.865, 0.9, 0.875, 0.91, 0.88])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as required by the spec, we do a 10-fold cross-validation\n",
    "def evaluate(test_db, trained_tree):\n",
    "  accuracies = [0.25] # this is the excepted accuracy for a pure random selector\n",
    "  optimal_tree = None\n",
    "  best_accuracy = 0.25\n",
    "  # so we first split the data into 10 parts\n",
    "  parts = np.split(test_db, 10, axis=0)\n",
    "  # in ten fold cross-validation, we use one as the test set, \n",
    "  # and the other for the train+validation sets\n",
    "  for i in range(10):\n",
    "    test = parts[i]\n",
    "    # train = np.concatenate([parts[j] for j in range(10) if j != i], axis=0)\n",
    "    # # build the tree\n",
    "    # tree, depth = decision_tree(train, 0)\n",
    "\n",
    "    # get the labels for the test samples\n",
    "    y = test[:, -1]\n",
    "    # get the predicted labels\n",
    "    y_pred = predict_batch(trained_tree, test[:, :-1])\n",
    "    # get the accuarcy\n",
    "    accuracy = np.mean(y == y_pred)\n",
    "    if (accuracies[-1] < accuracy):\n",
    "      optimal_tree = tree\n",
    "      best_accuracy = accuracy\n",
    "    # append regardless\n",
    "    accuracies.append(accuracy)\n",
    "    # lets do 6f, since the difference is too small to notice\n",
    "    print(f\"fold ${i+1}: accuracy = {accuracy:.6f}, tree depth = {depth}\")\n",
    "  print(f\"best accuracy: {best_accuracy}\")\n",
    "  return tree, accuracies\n",
    "evaluate(clean_data, tree)\n",
    "print(\"\\n\\n===== I am a separation line =====\\n\\n\")\n",
    "evaluate(noisy_data, tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879172f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
